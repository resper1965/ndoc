# Projeto Detalhado de Migra√ß√£o - ndocs

**Vers√£o**: 1.0  
**Data**: 2025-01-17  
**Status**: Planejamento Detalhado

## üìã Vis√£o Geral

Este documento detalha o projeto de migra√ß√£o do **ndocs** para uma plataforma SaaS completa com:
- **Vetoriza√ß√£o**: Supabase Vector (pgvector) para indexa√ß√£o e busca sem√¢ntica
- **RAG**: Prepara√ß√£o para chatbot futuro e indexa√ß√£o de documentos
- **Templates**: Pol√≠ticas, Procedimentos e Manuais
- **Ingest√£o**: PDF, DOCX ‚Üí Markdown com aplica√ß√£o autom√°tica de templates

## üéØ Objetivos da Migra√ß√£o

### Objetivos Principais

1. **Indexa√ß√£o Inteligente**: Todos os documentos vetorizados automaticamente
2. **Busca Sem√¢ntica**: Busca por significado, n√£o apenas palavras-chave
3. **Prepara√ß√£o para Chatbot**: RAG pronto para integra√ß√£o futura
4. **Templates Estruturados**: Pol√≠ticas, Procedimentos e Manuais padronizados
5. **Ingest√£o Autom√°tica**: Convers√£o autom√°tica de PDF/DOCX para Markdown

### Casos de Uso

- **Busca de Pol√≠ticas**: "Qual a pol√≠tica de f√©rias?"
- **Consulta de Procedimentos**: "Como fazer solicita√ß√£o de reembolso?"
- **Refer√™ncia de Manuais**: "Onde est√° o manual de onboarding?"
- **Chatbot Futuro**: Respostas baseadas em documentos indexados

---

## üèóÔ∏è Arquitetura da Solu√ß√£o

### Stack Escolhido

#### Vetoriza√ß√£o: **Supabase Vector (pgvector)** ‚úÖ

**Por qu√™?**
- ‚úÖ J√° integrado ao Supabase (sem infraestrutura adicional)
- ‚úÖ PostgreSQL nativo (performance e confiabilidade)
- ‚úÖ Suporte a busca por similaridade (cosine, L2, inner product)
- ‚úÖ √çndices HNSW para busca r√°pida
- ‚úÖ Gratuito at√© certo volume

**Especifica√ß√µes T√©cnicas:**
- **Modelo de Embedding**: OpenAI `text-embedding-3-small` (1536 dimens√µes)
- **Custo**: ~$0.02 por 1M tokens (muito barato)
- **Performance**: <100ms para busca em 10k documentos

### Fluxo de Dados

```
Upload (PDF/DOCX)
    ‚Üì
Convers√£o ‚Üí Markdown
    ‚Üì
Aplica√ß√£o de Template (Pol√≠tica/Procedimento/Manual)
    ‚Üì
Armazenamento no Supabase (tabela documents)
    ‚Üì
Chunking (dividir em peda√ßos de ~500 tokens)
    ‚Üì
Gera√ß√£o de Embeddings (OpenAI)
    ‚Üì
Armazenamento Vetorial (pgvector)
    ‚Üì
Indexa√ß√£o Completa ‚úÖ
```

### Fluxo de Busca (RAG)

```
Query do Usu√°rio
    ‚Üì
Gera√ß√£o de Embedding da Query
    ‚Üì
Busca por Similaridade (pgvector)
    ‚Üì
Retrieval dos Top-K Chunks
    ‚Üì
Context Injection para IA
    ‚Üì
Gera√ß√£o de Resposta (com cita√ß√µes)
```

---

## üìö Templates: Pol√≠ticas, Procedimentos e Manuais

### An√°lise do Pinexio

O [Pinexio](https://github.com/sanjayc208/pinexio) √© um template de documenta√ß√£o Next.js que usa:
- **MDX** para conte√∫do
- **Contentlayer** para indexa√ß√£o
- **Estrutura hier√°rquica** de documentos
- **Componentes reutiliz√°veis** (FolderTree, CodeTabs, etc.)

### Templates para ndocs

#### 1. **Template de Pol√≠tica**

**Estrutura:**
```markdown
---
title: [Nome da Pol√≠tica]
type: policy
category: [RH | Financeiro | TI | Operacional]
version: 1.0
effective_date: YYYY-MM-DD
review_date: YYYY-MM-DD
approver: [Nome]
status: [Ativa | Rascunho | Revogada]
---

# [Nome da Pol√≠tica]

## Objetivo
[Objetivo da pol√≠tica]

## Escopo
[Quem se aplica]

## Defini√ß√µes
[Termos importantes]

## Pol√≠tica
[Conte√∫do da pol√≠tica]

## Responsabilidades
[Quem √© respons√°vel pelo qu√™]

## Conformidade
[Como garantir conformidade]

## Refer√™ncias
[Links para documentos relacionados]

## Hist√≥rico de Revis√µes
| Data | Vers√£o | Mudan√ßas | Autor |
|------|--------|----------|-------|
| YYYY-MM-DD | 1.0 | Cria√ß√£o inicial | [Nome] |
```

#### 2. **Template de Procedimento**

**Estrutura:**
```markdown
---
title: [Nome do Procedimento]
type: procedure
category: [RH | Financeiro | TI | Operacional]
version: 1.0
effective_date: YYYY-MM-DD
owner: [Departamento]
status: [Ativo | Rascunho | Desativado]
---

# [Nome do Procedimento]

## Objetivo
[Objetivo do procedimento]

## Escopo
[Quando aplicar este procedimento]

## Respons√°veis
[Quem executa cada etapa]

## Materiais Necess√°rios
[Lista de materiais/ferramentas]

## Passo a Passo

### Passo 1: [Nome do Passo]
1. [A√ß√£o espec√≠fica]
2. [A√ß√£o espec√≠fica]
3. [A√ß√£o espec√≠fica]

### Passo 2: [Nome do Passo]
...

## Fluxograma
[Diagrama do processo - opcional]

## Exce√ß√µes
[Quando n√£o seguir o procedimento]

## Refer√™ncias
[Links para pol√≠ticas/procedimentos relacionados]

## Hist√≥rico de Revis√µes
| Data | Vers√£o | Mudan√ßas | Autor |
|------|--------|----------|-------|
| YYYY-MM-DD | 1.0 | Cria√ß√£o inicial | [Nome] |
```

#### 3. **Template de Manual**

**Estrutura:**
```markdown
---
title: [Nome do Manual]
type: manual
category: [Onboarding | Sistema | Processo]
version: 1.0
target_audience: [Novos funcion√°rios | Usu√°rios do sistema | Equipe]
status: [Ativo | Rascunho]
---

# [Nome do Manual]

## Introdu√ß√£o
[Contexto e prop√≥sito do manual]

## √çndice
1. [Se√ß√£o 1](#secao-1)
2. [Se√ß√£o 2](#secao-2)
...

## [Se√ß√£o 1]

### [Subse√ß√£o 1.1]
[Conte√∫do]

### [Subse√ß√£o 1.2]
[Conte√∫do]

## [Se√ß√£o 2]
...

## Gloss√°rio
| Termo | Defini√ß√£o |
|-------|-----------|
| Termo 1 | Defini√ß√£o 1 |
| Termo 2 | Defini√ß√£o 2 |

## FAQ
**P: Pergunta frequente?**  
R: Resposta.

## Recursos Adicionais
[Links para recursos externos]

## Contato
[Como obter ajuda]
```

---

## üóÑÔ∏è Estrutura de Banco de Dados

### Novas Tabelas

#### 1. `document_templates`

```sql
CREATE TABLE document_templates (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  organization_id UUID REFERENCES organizations(id) ON DELETE CASCADE,
  name VARCHAR(255) NOT NULL,
  type VARCHAR(50) NOT NULL CHECK (type IN ('policy', 'procedure', 'manual')),
  description TEXT,
  template_content TEXT NOT NULL, -- Template MDX com placeholders
  metadata_schema JSONB, -- Schema para frontmatter
  is_default BOOLEAN DEFAULT false,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- RLS
ALTER TABLE document_templates ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Users can view templates in their organization"
  ON document_templates FOR SELECT
  USING (
    organization_id IN (
      SELECT organization_id FROM organization_members
      WHERE user_id = auth.uid()
    )
    OR is_superadmin()
  );

CREATE POLICY "Admins can manage templates"
  ON document_templates FOR ALL
  USING (
    is_superadmin()
    OR is_orgadmin(organization_id, auth.uid())
  );
```

#### 2. `document_chunks`

```sql
CREATE TABLE document_chunks (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  document_id UUID REFERENCES documents(id) ON DELETE CASCADE,
  chunk_index INTEGER NOT NULL,
  content TEXT NOT NULL,
  token_count INTEGER,
  metadata JSONB, -- Informa√ß√µes adicionais (se√ß√£o, p√°gina, etc.)
  created_at TIMESTAMPTZ DEFAULT NOW(),
  UNIQUE(document_id, chunk_index)
);

-- RLS (herda da tabela documents)
ALTER TABLE document_chunks ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Users can view chunks of accessible documents"
  ON document_chunks FOR SELECT
  USING (
    document_id IN (
      SELECT id FROM documents
      WHERE organization_id IN (
        SELECT organization_id FROM organization_members
        WHERE user_id = auth.uid()
      )
      OR status = 'published'
    )
    OR is_superadmin()
  );
```

#### 3. `document_embeddings`

```sql
-- Habilitar extens√£o pgvector
CREATE EXTENSION IF NOT EXISTS vector;

CREATE TABLE document_embeddings (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  chunk_id UUID REFERENCES document_chunks(id) ON DELETE CASCADE,
  embedding vector(1536) NOT NULL, -- OpenAI text-embedding-3-small
  model VARCHAR(100) DEFAULT 'text-embedding-3-small',
  created_at TIMESTAMPTZ DEFAULT NOW(),
  UNIQUE(chunk_id)
);

-- √çndice HNSW para busca r√°pida
CREATE INDEX ON document_embeddings 
USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);

-- RLS (herda da tabela document_chunks)
ALTER TABLE document_embeddings ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Users can view embeddings of accessible chunks"
  ON document_embeddings FOR SELECT
  USING (
    chunk_id IN (
      SELECT id FROM document_chunks
      WHERE document_id IN (
        SELECT id FROM documents
        WHERE organization_id IN (
          SELECT organization_id FROM organization_members
          WHERE user_id = auth.uid()
        )
        OR status = 'published'
      )
    )
    OR is_superadmin()
  );
```

#### 4. `document_processing_jobs`

```sql
CREATE TABLE document_processing_jobs (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  document_id UUID REFERENCES documents(id) ON DELETE CASCADE,
  status VARCHAR(50) NOT NULL DEFAULT 'pending' 
    CHECK (status IN ('pending', 'processing', 'completed', 'failed')),
  stage VARCHAR(50), -- 'conversion', 'chunking', 'embedding', 'complete'
  error_message TEXT,
  metadata JSONB,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW(),
  completed_at TIMESTAMPTZ
);

-- RLS
ALTER TABLE document_processing_jobs ENABLE ROW LEVEL SECURITY;

CREATE POLICY "Users can view jobs for their documents"
  ON document_processing_jobs FOR SELECT
  USING (
    document_id IN (
      SELECT id FROM documents
      WHERE organization_id IN (
        SELECT organization_id FROM organization_members
        WHERE user_id = auth.uid()
      )
    )
    OR is_superadmin()
  );
```

### Atualiza√ß√µes na Tabela `documents`

```sql
-- Adicionar campos para templates
ALTER TABLE documents
ADD COLUMN template_id UUID REFERENCES document_templates(id),
ADD COLUMN document_type VARCHAR(50) CHECK (document_type IN ('policy', 'procedure', 'manual', 'other')),
ADD COLUMN is_vectorized BOOLEAN DEFAULT false,
ADD COLUMN vectorized_at TIMESTAMPTZ;
```

---

## üîÑ Pipeline de Processamento

### 1. Upload e Convers√£o

**Arquivo**: `src/lib/processing/convert-document.ts`

```typescript
export async function convertDocument(
  file: File,
  organizationId: string
): Promise<{
  content: string;
  metadata: Record<string, any>;
}> {
  const fileType = getFileType(file.name);
  
  switch (fileType) {
    case 'pdf':
      return await convertPDFToMarkdown(file);
    case 'docx':
      return await convertDOCXToMarkdown(file);
    case 'txt':
      return await convertTXTToMarkdown(file);
    case 'md':
      return await validateMarkdown(file);
    default:
      throw new Error(`Tipo de arquivo n√£o suportado: ${fileType}`);
  }
}
```

### 2. Aplica√ß√£o de Template

**Arquivo**: `src/lib/processing/apply-template.ts`

```typescript
export async function applyTemplate(
  content: string,
  templateId: string,
  metadata: Record<string, any>
): Promise<string> {
  const template = await getTemplate(templateId);
  
  // Extrair metadados do documento original
  const extractedMetadata = extractMetadata(content, template.metadata_schema);
  
  // Aplicar template
  const templatedContent = renderTemplate(
    template.template_content,
    { ...metadata, ...extractedMetadata }
  );
  
  // Combinar frontmatter + conte√∫do
  return combineFrontmatterAndContent(templatedContent, content);
}
```

### 3. Chunking

**Arquivo**: `src/lib/vectorization/chunk-document.ts`

```typescript
export function chunkDocument(
  content: string,
  options: {
    chunkSize?: number; // tokens
    chunkOverlap?: number; // tokens
    strategy?: 'sentence' | 'paragraph' | 'semantic';
  } = {}
): DocumentChunk[] {
  const {
    chunkSize = 500,
    chunkOverlap = 50,
    strategy = 'paragraph'
  } = options;
  
  // Dividir por par√°grafos (melhor para pol√≠ticas/procedimentos)
  if (strategy === 'paragraph') {
    return chunkByParagraphs(content, chunkSize, chunkOverlap);
  }
  
  // Dividir por senten√ßas (melhor para manuais)
  if (strategy === 'sentence') {
    return chunkBySentences(content, chunkSize, chunkOverlap);
  }
  
  // Dividir semanticamente (futuro)
  return chunkSemantically(content, chunkSize, chunkOverlap);
}
```

### 4. Gera√ß√£o de Embeddings

**Arquivo**: `src/lib/vectorization/generate-embeddings.ts`

```typescript
export async function generateEmbeddings(
  chunks: DocumentChunk[],
  organizationId: string
): Promise<Embedding[]> {
  const apiKey = await getOpenAIKey(organizationId);
  
  const embeddings = await Promise.all(
    chunks.map(async (chunk) => {
      const response = await openai.embeddings.create({
        model: 'text-embedding-3-small',
        input: chunk.content,
      });
      
      return {
        chunkId: chunk.id,
        embedding: response.data[0].embedding,
        model: 'text-embedding-3-small',
      };
    })
  );
  
  return embeddings;
}
```

### 5. Armazenamento Vetorial

**Arquivo**: `src/lib/vectorization/store-embeddings.ts`

```typescript
export async function storeEmbeddings(
  embeddings: Embedding[],
  supabase: SupabaseClient
): Promise<void> {
  const inserts = embeddings.map((emb) => ({
    chunk_id: emb.chunkId,
    embedding: emb.embedding,
    model: emb.model,
  }));
  
  const { error } = await supabase
    .from('document_embeddings')
    .insert(inserts);
  
  if (error) {
    throw new Error(`Erro ao armazenar embeddings: ${error.message}`);
  }
}
```

---

## üîç Sistema de Busca Sem√¢ntica

### Fun√ß√£o SQL para Busca

**Arquivo**: `supabase/migrations/YYYYMMDD_create_semantic_search_function.sql`

```sql
CREATE OR REPLACE FUNCTION semantic_search(
  query_embedding vector(1536),
  organization_id_filter UUID DEFAULT NULL,
  document_type_filter VARCHAR(50) DEFAULT NULL,
  match_threshold FLOAT DEFAULT 0.7,
  match_count INT DEFAULT 10
)
RETURNS TABLE (
  chunk_id UUID,
  document_id UUID,
  content TEXT,
  similarity FLOAT,
  document_title TEXT,
  document_type VARCHAR(50),
  chunk_index INTEGER
)
LANGUAGE plpgsql
AS $$
BEGIN
  RETURN QUERY
  SELECT
    de.chunk_id,
    dc.document_id,
    dc.content,
    1 - (de.embedding <=> query_embedding) AS similarity,
    d.title AS document_title,
    d.document_type,
    dc.chunk_index
  FROM document_embeddings de
  JOIN document_chunks dc ON de.chunk_id = dc.id
  JOIN documents d ON dc.document_id = d.id
  WHERE
    (organization_id_filter IS NULL OR d.organization_id = organization_id_filter)
    AND (document_type_filter IS NULL OR d.document_type = document_type_filter)
    AND (1 - (de.embedding <=> query_embedding)) >= match_threshold
  ORDER BY de.embedding <=> query_embedding
  LIMIT match_count;
END;
$$;
```

### API de Busca

**Arquivo**: `src/app/api/search/semantic/route.ts`

```typescript
export async function POST(request: NextRequest) {
  const { query, organizationId, documentType, limit = 10 } = await request.json();
  
  // Gerar embedding da query
  const queryEmbedding = await generateEmbedding(query);
  
  // Buscar no banco
  const { data, error } = await supabase.rpc('semantic_search', {
    query_embedding: queryEmbedding,
    organization_id_filter: organizationId || null,
    document_type_filter: documentType || null,
    match_threshold: 0.7,
    match_count: limit,
  });
  
  if (error) {
    return NextResponse.json({ error: error.message }, { status: 500 });
  }
  
  return NextResponse.json({ results: data });
}
```

---

## ü§ñ Prepara√ß√£o para Chatbot (RAG)

### Estrutura RAG

**Arquivo**: `src/lib/rag/query.ts`

```typescript
export async function queryRAG(
  question: string,
  context: {
    organizationId: string;
    documentType?: string;
    maxChunks?: number;
  }
): Promise<{
  answer: string;
  sources: Array<{
    documentId: string;
    documentTitle: string;
    chunkIndex: number;
    similarity: number;
  }>;
}> {
  // 1. Buscar chunks relevantes
  const relevantChunks = await semanticSearch(question, context);
  
  // 2. Construir contexto
  const contextText = relevantChunks
    .map((chunk, idx) => `[${idx + 1}] ${chunk.content}`)
    .join('\n\n');
  
  // 3. Gerar resposta com IA
  const answer = await generateAnswer(question, contextText);
  
  // 4. Extrair cita√ß√µes
  const sources = relevantChunks.map((chunk) => ({
    documentId: chunk.document_id,
    documentTitle: chunk.document_title,
    chunkIndex: chunk.chunk_index,
    similarity: chunk.similarity,
  }));
  
  return { answer, sources };
}
```

### Endpoint RAG (Futuro)

**Arquivo**: `src/app/api/rag/query/route.ts`

```typescript
export async function POST(request: NextRequest) {
  const { question, organizationId, documentType } = await request.json();
  
  const result = await queryRAG(question, {
    organizationId,
    documentType,
    maxChunks: 5,
  });
  
  return NextResponse.json(result);
}
```

---

## üìÖ Cronograma de Implementa√ß√£o

### Fase 1: Funda√ß√£o (Semana 1-2)

**Objetivo**: Configurar infraestrutura de vetoriza√ß√£o

- [ ] Habilitar pgvector no Supabase
- [ ] Criar migrations para novas tabelas
- [ ] Implementar fun√ß√µes SQL de busca
- [ ] Testes de performance

**Entreg√°veis**:
- ‚úÖ pgvector habilitado
- ‚úÖ Tabelas criadas
- ‚úÖ Fun√ß√µes SQL testadas

### Fase 2: Templates (Semana 3-4)

**Objetivo**: Criar sistema de templates

- [ ] Criar tabela `document_templates`
- [ ] Implementar templates de Pol√≠tica, Procedimento e Manual
- [ ] Interface para criar/editar templates
- [ ] Aplica√ß√£o autom√°tica de templates

**Entreg√°veis**:
- ‚úÖ 3 templates padr√£o criados
- ‚úÖ Interface de gerenciamento
- ‚úÖ Aplica√ß√£o autom√°tica funcionando

### Fase 3: Convers√£o e Chunking (Semana 5-7)

**Objetivo**: Implementar convers√£o ampla de documentos modernos e prepara√ß√£o para vetoriza√ß√£o

#### Tipos de Documentos Suportados

**Documentos de Texto:**
- [ ] PDF ‚Üí Markdown
- [ ] DOCX ‚Üí Markdown
- [ ] DOC (Word antigo) ‚Üí Markdown
- [ ] RTF ‚Üí Markdown
- [ ] ODT (OpenDocument) ‚Üí Markdown
- [ ] TXT ‚Üí Markdown
- [ ] MD/MDX ‚Üí Valida√ß√£o e sanitiza√ß√£o

**Documentos Estruturados:**
- [ ] HTML ‚Üí Markdown
- [ ] JSON ‚Üí Markdown (formatado)
- [ ] XML ‚Üí Markdown (formatado)
- [ ] CSV ‚Üí Markdown (tabela)

**Planilhas e Apresenta√ß√µes:**
- [ ] XLSX (Excel) ‚Üí Markdown (tabelas)
- [ ] PPTX (PowerPoint) ‚Üí Markdown (slides)

**Tarefas:**
- [ ] Sistema de upload (drag & drop)
- [ ] Detec√ß√£o autom√°tica de tipo de arquivo
- [ ] Conversores para cada tipo
- [ ] Aplica√ß√£o de templates
- [ ] Sistema de chunking
- [ ] Extra√ß√£o de metadados

**Entreg√°veis**:
- ‚úÖ Upload funcionando
- ‚úÖ Convers√£o de todos os tipos funcionando
- ‚úÖ Templates aplicados automaticamente
- ‚úÖ Chunking implementado

### Fase 4: Vetoriza√ß√£o (Semana 8-10)

**Objetivo**: Implementar pipeline completo de vetoriza√ß√£o

- [ ] Gera√ß√£o de embeddings
- [ ] Armazenamento vetorial
- [ ] Processamento ass√≠ncrono
- [ ] Status de processamento

**Entreg√°veis**:
- ‚úÖ Pipeline completo funcionando
- ‚úÖ Documentos vetorizados automaticamente
- ‚úÖ Dashboard de status

### Fase 5: Busca Sem√¢ntica (Semana 11-12)

**Objetivo**: Implementar busca sem√¢ntica

- [ ] API de busca sem√¢ntica
- [ ] Interface de busca
- [ ] Ranking e filtros
- [ ] Testes de performance

**Entreg√°veis**:
- ‚úÖ Busca sem√¢ntica funcionando
- ‚úÖ Interface de busca
- ‚úÖ Performance <100ms

### Fase 6: RAG (Semana 13-14)

**Objetivo**: Preparar para chatbot

- [ ] Implementar query RAG
- [ ] Context injection
- [ ] Cita√ß√µes e refer√™ncias
- [ ] API RAG (prepara√ß√£o)

**Entreg√°veis**:
- ‚úÖ RAG funcionando
- ‚úÖ API pronta para chatbot
- ‚úÖ Cita√ß√µes implementadas

---

## üìä M√©tricas de Sucesso

### Performance

- ‚úÖ **Busca sem√¢ntica**: <100ms para 10k documentos
- ‚úÖ **Vetoriza√ß√£o**: <30s por documento (m√©dio)
- ‚úÖ **Chunking**: <1s por documento

### Qualidade

- ‚úÖ **Precis√£o de busca**: >80% de relev√¢ncia
- ‚úÖ **Cobertura de templates**: 100% dos documentos com template
- ‚úÖ **Taxa de sucesso de convers√£o**: >95%

### Custo

- ‚úÖ **Embeddings**: <$0.10 por 1000 documentos
- ‚úÖ **Storage**: <$5/m√™s para 10k documentos

---

## üö® Riscos e Mitiga√ß√µes

### Risco 1: Performance de Busca

**Risco**: Busca lenta com muitos documentos

**Mitiga√ß√£o**:
- √çndices HNSW otimizados
- Cache de queries frequentes
- Limite de resultados

### Risco 2: Custo de Embeddings

**Risco**: Custo alto com muitos documentos

**Mitiga√ß√£o**:
- Usar modelo `text-embedding-3-small` (mais barato)
- Processamento em lote
- Cache de embeddings

### Risco 3: Qualidade de Convers√£o

**Risco**: PDF/DOCX mal formatados

**Mitiga√ß√£o**:
- M√∫ltiplas bibliotecas (fallback)
- Valida√ß√£o p√≥s-convers√£o
- Op√ß√£o de edi√ß√£o manual

---

## üìù Pr√≥ximos Passos

1. **Revisar e aprovar** este plano
2. **Criar branch** `feature/vectorization-migration`
3. **Iniciar Fase 1**: Configura√ß√£o de pgvector
4. **Seguir cronograma** fase por fase

---

**√öltima atualiza√ß√£o**: 2025-01-17

